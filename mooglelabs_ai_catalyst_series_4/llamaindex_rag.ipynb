{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76a473b1-7b7d-457b-bd97-18275c9a2500",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "186f9829-a96c-47ff-8a11-66a5cde97450",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index-readers-file pymupdf\n",
    "!pip install llama-index-vector-stores-postgres\n",
    "!pip install llama-index-embeddings-huggingface\n",
    "!pip install llama-index-llms-llama-cpp\n",
    "!pip install llama-index-llms-huggingface-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3fde414e-8081-45fc-b1c7-4f8bf224212f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-cpp-python\n",
    "!pip install llama-index-vector-stores-chroma\n",
    "!pip install llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2eae5b-33a3-4ec2-a470-a8ec80f75f1e",
   "metadata": {},
   "source": [
    "# Creds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a261f3c6-1cfb-40d4-af36-230395b91c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "hf_token = os.getenv(\"HUGGING_FACE_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d178e5-5487-43f5-9f53-073f8088e05c",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d46161b4-e30b-45c7-b069-fd2712b5ee4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core import Settings\n",
    "\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "import chromadb\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import time\n",
    "import os\n",
    "from typing import List, Optional\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# from llama_index.llms.ollama import Ollama\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aa62fc-50af-4818-b01b-12ffc1e3b094",
   "metadata": {},
   "source": [
    "# LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95e1b8b6-c766-4895-ab15-5a4393234753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the LLM you want to use\n",
    "\n",
    "# local inference\n",
    "llamacpp_llm = LlamaCPP(\n",
    "    model_path='gguf/Mistral-7B-Instruct-v0.3-Q8_0.gguf',  # download the GGUF model from huggingface\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=256,\n",
    "    context_window=4096,\n",
    "    generate_kwargs={},\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# ollama_llm = Ollama(model=\"llama3.1:latest\", request_timeout=30.0)\n",
    "\n",
    "# openai inference\n",
    "# openai_llm = OpenAI(model=\"gpt-4o-mini\", api_key=openai_api_key)\n",
    "# messages = [\n",
    "#     ChatMessage(\n",
    "#         role=\"system\", content=\"You are a pirate with a colorful personality\"\n",
    "#     ),\n",
    "#     ChatMessage(role=\"user\", content=\"What is your name\"),]\n",
    "\n",
    "# huggingface inference\n",
    "# hf_inf_llm=HuggingFaceInferenceAPI(model_name=\"mistralai/Mixtral-8x7B-Instruct-v0.1\", token=hf_token)\n",
    "# Settings.llm=HuggingFaceInferenceAPI(model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\", token=hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f09b95-c1cf-4ef7-b3a9-b3e0a24787ec",
   "metadata": {},
   "source": [
    "#### Huggingface inference API testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "474865b9-66a5-41eb-8683-ce79b350ed1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Python is a high-level, interpreted, interactive and object-oriented scripting language that is widely used for web development, scientific computing, data analysis, artificial intelligence, and more.\n",
      "\n",
      "Note: This answer is a brief summary of Python, if you want to know more about Python, you can check out the official Python documentation or other resources.\n"
     ]
    }
   ],
   "source": [
    "hf_inf_llm = HuggingFaceInferenceAPI(model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\", token=hf_token)\n",
    "\n",
    "completion_response = hf_inf_llm.complete(\"what is python. answer in one line\")\n",
    "print(completion_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6f35ef-c901-4920-8c4b-6341c4248b12",
   "metadata": {},
   "source": [
    "# Embedding generation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b9b6b73-fa1e-41c7-aef4-0af319659f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\", cache_folder='embed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38327d9-6ccb-4294-b606-71d51c6477a0",
   "metadata": {},
   "source": [
    "# Load and process your docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b622761-53b7-4931-a001-9d99aa6d7a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"data\").load_data() #make a folder named 'data' and pass your docs like pdf, docx, etc\n",
    "Settings.chunk_size = 100\n",
    "Settings.chunk_overlap = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd33d4e-c023-431b-92b6-9f2e10f0654f",
   "metadata": {},
   "source": [
    "# Generate embeddings and save to vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a74f629-d7a3-48b6-877e-171e08811b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "chroma_collection = db.get_or_create_collection(\"finance_agents\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context, embed_model=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ed9ccd-50c6-4192-a889-cdd66f0d6c80",
   "metadata": {},
   "source": [
    "#### Fetch the stored embedding collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "373e6bf9-2fc0-4711-bce8-736a8286bf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "db2 = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "chroma_collection2 = db2.get_or_create_collection(\"finance_agents\")\n",
    "vector_store2 = ChromaVectorStore(chroma_collection=chroma_collection2)\n",
    "index2 = VectorStoreIndex.from_vector_store(\n",
    "    vector_store2,\n",
    "    embed_model=embed_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe96b1d-cad9-49f5-9f65-76e1112ffb7c",
   "metadata": {},
   "source": [
    "# Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c766fd5-fc90-403b-9643-9484908ba634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open source based models prompt\n",
    "open_source_llm_template_str = (\"\"\"\n",
    "You are a Q&A assistant. \n",
    "First read the context. \n",
    "Then read the question\n",
    "Your goal is to find the question as accurately as possible based on the instructions and context provided.\n",
    "then generate your response in bullet point way to the user\n",
    "\n",
    "\n",
    "Context:\n",
    "{context_str}\n",
    "\n",
    " \n",
    "Question:\n",
    "{query_str}\n",
    "\"\"\")\n",
    "\n",
    "open_source_llm_template = PromptTemplate(open_source_llm_template_str)\n",
    "\n",
    "\n",
    "# openai based GPT model prompt\n",
    "openai_template_str = (\"\"\"\n",
    "You are a Q&A assistant. Your goal is to answer questions as\n",
    "accurately as possible based on the instructions and context provided.\n",
    " \n",
    "Context:\n",
    " \n",
    "{context_str}\n",
    " \n",
    "Question:\n",
    " \n",
    "{query_str}\n",
    "\"\"\")\n",
    "\n",
    "openai_template = PromptTemplate(refine_template_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d3e573-ce25-4c66-8c89-c153d6b4b97d",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abd431d-243d-4a2c-97fb-5a201ddc6eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ques = input('Your question goes here: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91652ca6-b3ae-4478-8e67-adb7fed36440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "AutoGen is a generic framework designed to build diverse applications of various complexities and Language Model (LLM) capacities. It is particularly useful in domains such as mathematics, coding, question-answering, operations research, online decision-making, and entertainment. The framework streamlines and consolidates multi-agent workflows using multi-agent conversations to reduce the effort required for developers to create complex LLM applications.\n",
      "\n",
      "Time taken to generate:  40.642993450164795\n"
     ]
    }
   ],
   "source": [
    "a=time.time()\n",
    "\n",
    "query_engine = index2.as_query_engine(text_qa_template=open_source_llm_template,  # pass your prompt here\n",
    "                                      llm=llamacpp_llm,  # pass your LLM here\n",
    "                                      streaming=True)\n",
    "\n",
    "response = query_engine.query(ques)\n",
    "# print(response)  # uncomment in case using LLM from hugginface inferenceAPI as it doesn't support steaming\n",
    "response.print_response_stream()  # comment this while using LLM from huggingface inference API\n",
    "\n",
    "b=time.time()\n",
    "c=b-a\n",
    "formatted_time = float(format(c, \".2f\"))\n",
    "print('\\n\\n\\n\\nTime taken to generate: ', formatted_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f200be-7ded-4203-b63d-22e5b36db0c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6f8221-6fee-4cb8-bb1f-3507ddc878e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3684db1-e990-4afa-bd11-3848ac8aef00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54ef33a1-6298-45e0-a70a-f1ec3871cf7a",
   "metadata": {},
   "source": [
    "# Generated examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fe3ec00a-9b4d-493f-ac0d-97aa8f1cfe97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "In Chapter 2, Financial Performance, the report discusses the financial achievements of the company for the year 2023. The key points highlighted include a 12% growth in revenue compared to the previous year, reaching $3.2 billion. This growth is attributed to increased sales from higher-margin products and cost optimization initiatives. Additionally, the gross profit margin improved to 45%. This chapter provides an in-depth analysis of the financial performance of the company for the year.\n",
      "\n",
      "Time taken to generate:  59.90000009536743\n"
     ]
    }
   ],
   "source": [
    "# LlamaCPP - Tell me about chapter 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6b89eeaa-25a7-48ab-be28-e48cbd56c577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "The report for the year 2024, as per the Financial Report, presents a cautiously optimistic outlook with an expected revenue growth of 8%. This is a moderation from the extraordinary factors that contributed to the 12% revenue increase seen in the year 2023.\n",
      "\n",
      "Time taken to generate:  43.412781953811646\n"
     ]
    }
   ],
   "source": [
    "# LlamaCPP - what is the financial report for year 2024?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "408f8094-a582-4d10-824c-6aceb112cf7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "The financial year 2023 was a year of robust growth for our company, with a revenue increase of 12% compared to the previous year.\n",
      "\n",
      "Time taken to generate:  33.57563900947571\n"
     ]
    }
   ],
   "source": [
    "# LlamaCPP - how was the financial year 2023?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a3d5595c-279e-49b9-be61-929997f7e23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "Response:\n",
      "According to the financial report, the net income was $500 million, representing a 15% year-over-year increase.\n",
      "\n",
      "\n",
      "Time taken to generate:  1.076716423034668\n"
     ]
    }
   ],
   "source": [
    "# Mixtral 8x7B MoE Huggingface - What is the net income?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c66c2cf3-f59e-4d4d-b0e4-021ce2861ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "Response:\n",
      "AutoGen is a generic framework for building diverse applications of various complexities and Large Language Model (LLM) capacities. It aims to streamline and consolidate multi-agent workflows using multi-agent conversations, thereby reducing the effort required for developers to create complex LLM applications across various domains. \n",
      "\n",
      "Please let me know if you need any further assistance.  I'll be happy to help. \n",
      "\n",
      "\n",
      "Time taken to generate:  2.2069997787475586\n"
     ]
    }
   ],
   "source": [
    "# Mixtral 8x7B MoE Huggingface - What is Autogen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e698d95b-51f1-49ce-b9b8-2da7fc71ee0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      "$500 million\n",
      "\n",
      "Time taken to generate:  19.408074378967285\n"
     ]
    }
   ],
   "source": [
    "# LlamaCPP - What is the net income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e0b009e1-68c5-4956-94b5-89dd9fbcb5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "$500 million. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Please let me know if you want me to do anything else.\n",
      "\n",
      "\n",
      "Time taken to generate:  1.2758889198303223\n"
     ]
    }
   ],
   "source": [
    "# Llama 3 huggingface - What is the net income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5e6e6c55-a724-4bc8-ba2a-4d9619c4b2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The net income mentioned in the financial report is $500 million, representing a 15% increase year-over-year.\n",
      "\n",
      "Time taken to generate:  1.0434603691101074\n"
     ]
    }
   ],
   "source": [
    "# OpenAI - what is the net income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "53117abc-84e3-4bc5-8037-6b2e7693c6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoGen is a generic framework designed to facilitate the development of diverse applications with various complexities and capacities of Large Language Models (LLMs). It aims to streamline and consolidate multi-agent workflows using multi-agent conversations to reduce the effort required for developers to create complex LLM applications across different domains. Empirical studies have shown the effectiveness of the AutoGen framework in various example applications, including mathematics, coding, question answering, operations research, online decision-making, entertainment, and more.\n",
      "\n",
      "Time taken to generate:  2.124453544616699\n"
     ]
    }
   ],
   "source": [
    "# OpenAI - what is autogen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5b88e8-5239-4675-8e40-0aa9f3efa182",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aa7951-44f3-493c-9a50-58f44e0233a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
